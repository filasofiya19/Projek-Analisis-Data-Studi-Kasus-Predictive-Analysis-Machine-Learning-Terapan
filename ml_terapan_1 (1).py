# -*- coding: utf-8 -*-
"""ML Terapan 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14sMPnmwf6mG5Zlq9topsj1oy-hesChE4

# **1. Import Library**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency
from sklearn.preprocessing import PowerTransformer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix

"""# **2. Load Dataset**"""

# Import Data
data = pd.read_csv('https://raw.githubusercontent.com/filasofiya19/Projek-Analisis-Data-dengan-Neural-Network-online-shoppers-intention-dengan-Deep-Neural-Network-/main/online_shoppers_intention.csv')
print(data)

"""# **3. Data Understanding**

**Online Shoppers Purchasing Intention Dataset**

*Tentang Dataset*

Dataset ini mencakup 18 fitur dari 12.330 sampel.
Setiap sesi mewakili pengguna yang berbeda selama periode 1 tahun untuk menghindari bias terkait kampanye tertentu, hari istimewa, profil pengguna, atau periode waktu.


Dataset ini terdiri dari 10 fitur numerik dan 8 fitur kategorikal.

Fitur 'Revenue' merupakan label kelas.

```
| Fitur                   | Deskripsi                                                                                                                                                                   |
|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Administrative          | Jumlah halaman administratif yang dikunjungi selama sesi.                                                                                                                 |
| Administrative_Duration | Total waktu yang dihabiskan di halaman administratif selama sesi.                                                                                                           |
| Informational           | Jumlah halaman informasi yang dikunjungi selama sesi.                                                                                                                     |
| Informational_Duration  | Total waktu yang dihabiskan di halaman informasi selama sesi.                                                                                                               |
| ProductRelated          | Jumlah halaman terkait produk yang dikunjungi selama sesi.                                                                                                                 |
| ProductRelated_Duration | Total waktu yang dihabiskan di halaman terkait produk selama sesi.                                                                                                          |
| BounceRates             | Persentase pengunjung yang meninggalkan situs setelah hanya melihat satu halaman.                                                                                           |
| ExitRates               | Persentase tampilan halaman yang merupakan halaman terakhir dalam sesi.                                                                                                     |
| PageValues              | Nilai rata-rata untuk halaman yang dikunjungi sebelum menyelesaikan transaksi e-commerce.                                                                                  |
| SpecialDay              | Menunjukkan seberapa dekat tanggal kunjungan dengan hari istimewa tertentu, dengan nilai maksimum mendekati hari istimewa.                                                   |
| Month                   | Bulan dalam setahun ketika kunjungan terjadi.                                                                                                                               |
| OperatingSystems        | Sistem operasi yang digunakan oleh pengunjung (misalnya Windows, Mac, Linux).                                                                                               |
| Browser                 | Browser yang digunakan oleh pengunjung (misalnya Chrome, Firefox, Safari).                                                                                                  |
| Region                  | Wilayah geografis tempat pengunjung berasal.                                                                                                                                |
| TrafficType             | Jenis trafik yang mengarah ke situs (misalnya organik, referral, direct).                                                                                                    |
| VisitorType             | Tipe pengunjung (Returning_Visitor atau New_Visitor).                                                                                                                        |
| Weekend                 | Menunjukkan apakah kunjungan terjadi pada akhir pekan atau pada hari biasa.                                                                                                 |
| Revenue                 | Label kelas yang menunjukkan apakah transaksi terjadi selama sesi (True atau False).                                                                                       |

```

## **3.1 Cek Tipe Data**
"""

# Cek Tipe Data
data.info()

"""Dataset ini memiliki 2 tipe data bollean, 7 tipe data float, 7 tipe data integer, 2 tipe data object.

## **3.2 Cek Dimensi Data**
"""

# Cek dimensi data
data.shape

"""( data memiliki 12330 records (baris) dan 18 feature(kolom))"""

# Cek 5 data teratas
data.head()

#Cek 5 data terbawah
data.tail()

"""## **3.3 Pengelompokan Tipe Data**

Pengelompokan Tipe Data :

Month, OperatingSystems, Browser, Region, TrafficType, visitor type, Weekend, Revenue adalah data kategorik. Dengan OperatingSystems, Browser, Region, TrafficType pada data sudah di encoding, untuk data visitor type, Weekend, Revenue nanti akan dilakuakn data transformation dan kita perlu encoding .

Administrative, Administrative_Duration, Informational, Informational_Duration, ProductRelated, ProductRelated_Duration, BounceRates, ExitRates, PageValues, SpecialDay adalah Data Numerik.

Revenue: Variabel target atau label yang menunjukkan apakah pembeli berbelanja online atau tidak. Label ini bernilai (True) untuk pembeli yang berbelanja online dan (False) untuk pembeli yang tidak berbelanja online.
Revenue akan dilakukam data transformation jadi akan di encoding dengan true = 1 dan false = 0.
"""

# membagi feature berdasarkan value numerik dan kategori

cats = ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend', 'Revenue']
nums = ['Administrative', 'Administrative_Duration','Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',
        'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']

"""## **3.4 Cek Missing Value**"""

# Cek Missing Value
data.isnull().sum()

"""Tidak ada missing value dalam data ini, sehingga tidak dilakukan handling missing value.

## **3.5 Cek Data Duplicate**
"""

# cek duplikat
data.duplicated().any()

data.duplicated().sum()

"""Terdapat 125 baris yang duplikat

## **3.6 Deskripsi Fitur Target**
"""

# melihat nilai revenue yang True dan False
revenue_col = data['Revenue'].value_counts().reset_index()
revenue_col.columns = ['Revenue', 'count']
revenue_col['%'] = (revenue_col['count']/revenue_col['count'].sum()*100).round(2)
revenue_col

"""Dari revenue (total transaksi), hanya 1908 transaksi yang berhasil dan menghasilkan pembeli."""

#   Visulisasi Revenue
x = revenue_col.plot(kind='bar', x='Revenue', y='%', color=sns.color_palette('tab10'))
plt.title('Persentase revenue', fontweight='bold')
plt.xticks(rotation=360)
for p in plt.gca().patches:
    plt.gca().annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                       ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=12)
plt.legend('')

"""## **3.7 Statistika Deskriptif Data**"""

# Statistika Deskripstif
data.describe()

"""# **4. Data Preparation**

## **4.1 Mengatasi Data Duplikat**
"""

data[data.duplicated(keep=False) == True]

# menghapus duplikat
print(f'Jumlah Baris yang Duplikat SEBELUM dihapus {data.duplicated().sum()}')
data.drop_duplicates(inplace=True)
print(f'Jumlah Baris yang Duplikat SETELAH dihapus {data.duplicated().sum()}')

# inisiasi update datadrame

data = data.drop_duplicates().reset_index(drop = True)

data.nunique()

# melihat nilai unique disetiap feature
for col in data.columns:
    print('{} mempunyai {} nilai unique : {}'.format(col, data[col].nunique(), data[col].dtypes))
    if data[col].dtypes == 'int64' or data[col].dtypes == 'float64' or data[col].dtypes == 'bool' or data[col].dtypes == 'object':
        print('{} nilai : {}'.format(col,data[col].unique()))

"""* Terdapat 2 bulan yang hilang yaitu Januari dan April.
* Nilai Other pada VisitorType.
* Fitur kategorikal seperti OperatingSystems, Browser, Region, TrafficTipe telah dilakukan encoding.

## **4.2 Statistika Deskriptif Data Setelah Data Duplikat Dihilangkan**
"""

# Statistika Deskripstif
data.describe()

"""* Mayoritas dataset memiliki angka **Mean < Median**, artinya distribusi data cenderung positively-skewed.
* Pada fitur yang menunjukkan **traffic website** seperti 'Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', dan 'PageValues **mayoritas memiliki nilai yang menumpuk di angka 0.**
"""

# melihat deskriptif statistik kolom kategorik
data_cat = data[['Month', 'VisitorType', 'Weekend', 'Revenue', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']].astype('object').describe().T
data_cat['% freq'] = (data_cat['freq']/data_cat['count']*100)
data_cat

"""* Jumlah pengunjung web tertinggi ada pada bulan Mei.
* Mayoritas 85% pelanggan adalah Returning Visitor.
* Sebanyak 84% pengunjung website tidak purchase.
* Mayoritas pelanggan berasal dari Region 1 dan berselancar di website menggunakan OperatingSystems, Browser, TraficType jenis 2.

## **4.3 Cek Outlier Data**
"""

# cek distribusi dan outlier menggunakan boxplot
plt.figure(figsize=(15, 20))
for i in range(0, len(nums)):
    plt.subplot(5, 5, i+1)
    sns.boxplot(y=data[nums[i]], orient='v')
    plt.tight_layout()

#Cek outlier data
filtered_entries = np.array([True] * len(data))

print(f'Jumlah baris sebelum memfilter outlier: {len(data)}')

for col in nums:
    zscore = abs(stats.zscore(data[col])) # menghitung absolute z-scorenya
    filtered_entries = (zscore < 3) & filtered_entries # keep yang kurang dari 3 absolute z-scorenya

data1 = data[filtered_entries]
print(f'Jumlah baris setelah memfilter outlier: {len(data1)}')
print(f'Persentase outlier: {round((len(data)-len(data1))/len(data)*100, 2)}%')

"""Presentase outlier dalam data 17.90%, nilai tersebut cukup besar, maka outlier tidak dihilangkan. Tidak dilakukan handle juga karena outlier ini bukan dari kesalahan dalam pengambilan data.

## **4.4 Data Transformation**

Transformasi feature tidak menggunakan log karena data memiliki banyak value dengan nilai 0. PowerTransformer Yeo-Johnson dipilih untuk membuat distribusi lebih mendekati normal (Guassian) dan mendukung value data memiliki nilai positif atau negatif.
"""

# transformasi data
for x in nums:
    pt = PowerTransformer(method='yeo-johnson')
    data[x] = pt.fit_transform(data[x].to_frame())

# cek distribusi data

features = nums
plt.figure(figsize=(17, 17))
for i in range(0, len(nums)):
    plt.subplot(5, 5, i+1)
    sns.kdeplot(x=data[features[i]])
    plt.xlabel(features[i])
    plt.tight_layout()

"""## 4.5 Feature Encoding

### 4.5.1 Encoding Fitur Visitor Type
"""

data['VisitorType'].value_counts()

"""Disini Other akan dianggap sebagai returning visitor (modus), akan dilakukan one hot encoding dimana Returning Visitor : 1, dan New Vistor : 0."""

data['VisitorType'].replace('Other', 'Returning_Visitor', inplace=True)

data['VisitorType'] = data['VisitorType'].map({'New_Visitor': 0, 'Returning_Visitor': 1})

"""### **4.5.2 Encoding Fitur Revenue**"""

data['Revenue'].value_counts()

"""Disini akan dilakukan one hot encoding, Dengan True: 1, dan False : 0."""

data['Revenue'] = data['Revenue'].map({False: 0, True: 1})

"""### **4.5.3 Encoding Fitur Weekend**"""

data['Weekend'].value_counts()

"""Disini akan dilakukan one hot encoding, Dengan True: 1, dan False : 0."""

data['Weekend'] = data['Weekend'].map({False: 0, True: 1})

"""### **4.5.4 Encoding Fitur Month**"""

data['Month'].value_counts()

"""Fitur Month akan dilakukan label encoding, sesuai index dari bulan paling banyak"""

# Menghitung jumlah kemunculan setiap bulan
month_counts = data['Month'].value_counts().reset_index()
month_counts.columns = ['Month', 'Count']

# Membuat urutan angka untuk setiap bulan berdasarkan peringkat frekuensi
month_counts['Month_Index'] = month_counts['Count'].rank(ascending=False).astype(int)

# Menampilkan hasil peringkat bulan
print("Peringkat bulan berdasarkan frekuensi:")
print(month_counts)

# Membuat dictionary untuk mapping bulan ke angka
mapping_month = month_counts.set_index('Month')['Month_Index'].to_dict()

# Menampilkan dictionary mapping
print("Mapping bulan ke angka:")
print(mapping_month)

# Mengonversi kolom 'Month' menjadi angka menggunakan mapping
data['Month'] = data['Month'].map(mapping_month)

"""**Melihat Sample Data setelah dilakukan Encoding**"""

data.sample(5)

"""**Melihat Tipe Data setelah dilakukan Encoding**"""

data.info()

"""# **5. Analisis Univariat dan Visualisasi Data**

## **5.1 Analisis Univariat Visitor Type dengan Revenue dan Visulisasinya**
"""

data['VisitorType'] = data['VisitorType']
# Menghitung statistik yang diperlukan
v = data.groupby(['VisitorType', 'Revenue'])['VisitorType'].count().reset_index(name='count')
v['percentage'] =  (100 * v['count'] / v.groupby('VisitorType')['count'].transform('sum')).round(2)

# Membuat pivot table
pvt = v.pivot_table(index='VisitorType', columns='Revenue', values=['count', 'percentage'])

# Membuat dictionary untuk mapping bilangan bulan ke nama bulan
v_mapping_reverse = {0:'New_Visitor', 1:'Returning_Visitor'}

# Mengganti nama indeks dengan nama bulan yang sesuai
pvt.index = pvt.index.map(v_mapping_reverse)
# Menampilkan hasil
pvt

pvt_count = v.pivot_table(index='VisitorType', columns='Revenue', values='count')

ax = pvt_count.plot(kind='barh', stacked=True)
plt.title('Persentase Revenue dengan Visitor Type', fontweight='bold')
plt.xticks(rotation=360)
plt.xlabel('jumlah')
plt.legend(loc='lower right', fontsize=12)
y_labels = ['New Visitor', 'Returning Visitor']  # Replace with your desired labels
ax.set_yticklabels(y_labels)

"""Dapat dilihat bahwa pada setahun terakhir jumlah kunjungan dan revenue pengunjung dari new visitor memiliki perbandingan yang cukup rendah di banding dengan returning visitor.

## **5.2 Analisis Univariat Month dengan Revenue dan Visualisasinya**
"""

data['Month'] = data['Month']
# Menghitung statistik yang diperlukan
m = data.groupby(['Month', 'Revenue'])['Month'].count().reset_index(name='jumlah')
m['persentase'] =  (100 * m['jumlah'] / m.groupby('Month')['jumlah'].transform('sum')).round(2)

# Membuat pivot table
pvt = m.pivot_table(index='Month', columns='Revenue', values=['jumlah', 'persentase'])

# Membuat dictionary untuk mapping bilangan bulan ke nama bulan
month_mapping_reverse = {10:'Feb', 3:'Mar', 1: 'May', 9:'June', 8:'Jul',7:'Aug', 6:'Sep', 5:'Oct',2:'Nov',4:'Dec'}  # Sesuaikan dengan kebutuhan Anda

# Mengganti nama indeks dengan nama bulan yang sesuai
pvt.index = pvt.index.map(month_mapping_reverse)
# Menampilkan hasil
pvt

pvt_count = m.pivot_table(index='Month', columns='Revenue', values='jumlah')
val =['May', 'Nov', 'Mar', 'Dec', 'Oct', 'Sep', 'Aug', 'Jul', 'June', 'Feb']

ax = pvt_count.plot(kind='line', figsize=(15,5))
plt.title('Total Visitor per Month vs Revenue', fontweight='bold')

# plt.xticks(rotation=360)
plt.xlabel('Count')
plt.legend(loc='lower right', fontsize=12)
plt.grid(color='darkgray', linestyle=':', linewidth=0.5)
plt.legend(loc='upper right', fontsize=12)

ax.set_xticks(np.arange(len(pvt_count)) + 1)
ax.set_xticklabels(val)

plt.axvline(x=1, color='darkgrey', ls='--', lw=1.5)
plt.axvline(x=2, color='darkgrey', ls='--', lw=1.5)

"""Dapat dilihat bahwa trafik jumlah kunjungan pelanggan setiap bulan memiliki jumlah yang paling tinggi pada bulan Mei dan di susul pada bulan November. Namun pada bulan Mei tingginya trafik tidak diikuti dengan tingginya angka Revenue yang hanya menghasilkan 11%. Sedangkan pada bulan November merupakan bulan yang memiliki cukup banyak pengunjung dengan nilai Revenue bulanan yang paling tinggi, yaitu mencapai 25%.

## **5.3 Analisis Univariat Region dengan Revenue dan Visualisasinya**
"""

reg = data.copy()
reg = reg.groupby(['Region', 'Revenue'])['Region'].count()
reg = reg.reset_index(name='jumlah')
reg['persentase'] =  (100 * reg['jumlah'] / reg.groupby('Region')['jumlah'].transform('sum')).round(2)

pvt = reg.pivot_table(index='Region', columns='Revenue', values=['jumlah', 'persentase'])
pvt

pvt_count = reg.pivot_table(index='Region', columns='Revenue', values='jumlah')

ax = pvt_count.plot(kind='bar', stacked=True)
plt.title('Visitor Region vs Revenue', fontweight='bold')
plt.xticks(rotation=360)

"""Kunjungan pelanggan didominasi pada region "1" dengan Revenue Rate yang cukup tinggi.

## **5.4 Analisis Univariat Weekend dengan Revenue**
"""

w = data.copy()
w = w.groupby(['Weekend', 'Revenue'])['Region'].count()
w = w.reset_index(name='jumlah')
w['persentase'] =  (100 * w['jumlah'] / w.groupby('Weekend')['jumlah'].transform('sum')).round(2)

pvt = w.pivot_table(index='Weekend', columns='Revenue', values=['jumlah', 'persentase'])
pvt

pvt_count = w.pivot_table(index='Weekend', columns='Revenue', values='jumlah')

ax = pvt_count.plot(kind='bar', stacked=True)
plt.title('Weekend vs Revenue', fontweight='bold')
plt.xticks(rotation=360)

pvt_count = w.pivot_table(index='Weekend', columns='Revenue', values='persentase')

ax = pvt_count.plot(kind='barh', stacked=True)
plt.title('Persentase Revenue Coversion Rate saat Weekend', fontweight='bold')
plt.xticks(rotation=360)

"""Pada dasarnya presentase kunjungan  Weekend dan Weekday tidak terlalu berbeda signifikan. Namun Weekday memiliki nilai yang lebih tinggi. Namun hal ini bisa terjadi dikarenakan memang jumlah hari di Weekday lebih banyak daripada Weekend atau memang pengunjung memang lebih sering mengunjungi website di hari-hari weekday.

# **6. Analisis Multivariat dan Visualisasinya**

## **6.1 Hubungan Antar Fitur Numerikal**
"""

# numerikal dengan target
nums_dan_revenue = data[nums]
nums_dan_revenue['Revenue'] = data['Revenue']
nums_dan_revenue.T

plt.figure(figsize = (20,20))
sns.pairplot(nums_dan_revenue, hue = 'Revenue')

"""pairplot ini menunjukkan hubungan antara pasangan variabel. Warna plot akan diberikan berdasarkan nilai dari kolom 'Revenue', yaitu apakah online shopers berbelanja atau tidak."""

# analisi menggunakan heatmap
plt.figure(figsize = (8,8))
sns.heatmap(data[nums].corr(), cmap = 'coolwarm', annot = True, fmt = '.2f')

"""* Fitur **ExitRates** dan **BounceRates** memiliki nilai multikolinieritas yang tinggi 0.9, kedua kolom ini pun berhubungan satu sama lain sehingga dapat di drop salah satunya.
* Terdapat beberapa **fitur-fitur** yang kemungkinan **redundan** karena memiliki korelasi yang cukup tinggi diantaranya **ProductRelated dengan ProductRelated_Duration, Adminisitrative dengan Adminisitrative_Duration, Informational dengan Informational_Duration**, dan begitu pula **BounceRates dengan ExitRates**. Dalam tahap data prepocessing feature-feature tersebut dapat di drop ataupun dipilih salah satu.
* Kolom PageValues ternyata memiliki korelasi tinggi dengan Revenue (0.49).
* Kolom **BounceRates** dengan beberapa kolom lain, **ExitRates** dengan beberapa kolom lain, dan page values dengan beberapa kolom lain berkumpul di bawah dan samping kiri cenderung membentuk pola logaritmik. Itu artinya, Apabila kolom **BounceRates** dan kolom **Informational_Duration** berhubungan secara logaritmik, **semakin besar nilai BounceRates, nilai Informational_Duration semakin kecil secara logaritmik**.

## 6.2 **Hubungan Antara Fitur Kategorikal**
"""

tips_categorical = data[cats]
tips_categorical

"""Disini digunakan uji chi-kuadrat pada tabel kontingensi. Uji ini digunakan untuk menguji independensi antara dua variabel kategorikal."""

def cramers_V(var1, var2):
    crosstab = np.array(pd.crosstab(var1, var2, rownames = None, colnames = None))
    stat = chi2_contingency(crosstab)[0]
    obs = np.sum(crosstab)
    mini = min(crosstab.shape)-1
    return (np.sqrt(stat/(obs*mini)))

rows = []
for var1 in tips_categorical:
    col = []
    for var2 in tips_categorical:
        cramers = cramers_V(tips_categorical[var1], tips_categorical[var2])
        col.append(round(cramers,2))
    rows.append(col)

cramers_result = np.array(rows)
tabel_corelation_categorical = pd.DataFrame(cramers_result, columns = tips_categorical.columns, index = tips_categorical.columns)

tabel_corelation_categorical

plt.figure(figsize = (8,8))
sns.heatmap(tabel_corelation_categorical, cmap = 'coolwarm', annot = True, fmt = '.2f')

"""* **Browser** dan **OperatingSystems** memiliki korelasi yang cukup tinggi.
* **Browser** dan **TrafficType** juga memiliki korelasi yang **tinggi**.
Nilai korelasi **antar kolom** dengan kolom **Revenue** cenderung **rendah**.

## **6.3 Hubungan Antara Fitur Numerikal dan Kategorikal (Semua Fitur)**
"""

#Correlation Table
corr = data.corr()
corr = (corr)

corr

correlation_with_revenue = data.corr()['Revenue'].sort_values(ascending=False)
print(correlation_with_revenue)

"""* Output ini adalah nilai korelasi antara variabel 'Revenue' (kolom target) dengan setiap variabel lain dalam dataset. Nilai korelasi ini berkisar antara -1 hingga 1.

* Korelasi Positif: Nilai korelasi mendekati 1 menunjukkan korelasi positif yang kuat. Misalnya, PageValue memiliki korelasi sebesar 0.491894 dengan 'Revenue'. Ini menunjukkan bahwa ada hubungan positif yang cukup kuat antara PageValue dan Revenue (online shoppers berbelanja).

* Korelasi Negatif: Nilai korelasi mendekati -1 menunjukkan korelasi negatif yang kuat. Misalnya Exit Rates dan Bouce Rates memiliki hubungan negatif yang cukup kuat antara Exit Rates dan Revenue (online shoppers berbelanja).

* Korelasi Dekat dengan 0: Nilai korelasi mendekati 0 menunjukkan korelasi yang lemah atau tidak ada korelasi yang signifikan antara variabel tersebut dengan 'Revenue'. Dalam kasus ini, variabel 'Browser' dan 'Weekend' memiliki nilai korelasi yang cukup rendah dengan 'Revenue' (kurang dari 0.1), yang menunjukkan hubungan yang lemah dengan kemungkinan Revenue (online shoppers berbelanja).
"""

corrmat = data.loc[:, data.columns != 'revenue'].corr()
top_corr_features = corrmat.index

plt.figure(figsize=(15,15))
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""* Fitur **ExitRates** dan **BounceRates** memiliki nilai multikolinieritas yang tinggi 0.9, kedua kolom ini pun berhubungan satu sama lain sehingga dapat di drop salah satunya.
* Terdapat beberapa fitur-fitur yang **redundan** karena memiliki korelasi yang cukup tinggi diantaranya **ProductRelated** dengan **ProductRelated_Duration**, **Adminisitrative**dengan **Adminisitrative_Duration**, **Informational** dengan **Informational_Duration**, dan begitu pula **BounceRates** dengan **ExitRates**. Dalam tahap data prepocessing feature-feature tersebut dapat di drop ataupun dipilih salah satu.
* Kolom **PageValues** memiliki korelasi tinggi dengan Revenue (0.49)
* **Browser** dan **OperatingSystems** memiliki korelasi yang cukup tinggi.
* **Browser** dan **TrafficType** juga memiliki korelasi yang **tinggi**.
Nilai korelasi **antar kolom** dengan kolom **Revenue** cenderung **rendah**.

# **7. Feature Selection**
"""

# memilih feature dengn korelasi tinggi dengan Revenue

x = corrmat['Revenue']
result = x[(x>0.05)|(x<-0.05)] # korelasi lebih dari 0.05 dan kuarng dari -0,05
result

"""Berdasar, nilai korelasi variabel dengan label revenue, maka fitur yang dipilih adalah Administrative, Administrative_Duration, Informational, Informational_Duratiom, ProductRelated, ProductRelated_Duration, BounceRates, ExitRates, PageValues, SpecialDay, VisitorType.

Tetapi beberapa variabel diatas ada yang redundan yaitu

* Administrative, Administrative_Duration dipilih Administrative_Duration,
* Informational, Informational_Duratiom, dipilih  Informational_Duration,
* ProductRelated, ProductRelated_Duration, dipilih ProductRelated_Duration

Kemudian ada variabel yang multikolinearitas nya tinggi yaitu BounceRates, ExitRates jadi dipilih salah satu yaitu ExitRates.
"""

# Menghapus kolom yang tidak dibutuhkan
shopping_clean = data.drop(['Administrative','Informational','ProductRelated','BounceRates','Month','Browser','OperatingSystems','Region','TrafficType','Weekend'], axis=1)

"""# **8. Splitting Data**"""

X = shopping_clean.drop('Revenue', axis=1)
y = shopping_clean['Revenue']
X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42, test_size=.2)

"""* Disini variabel X yang akan berisi fitur-fitur dari data, kecuali kolom 'Revenue', yang akan digunakan untuk membuat prediksi.

* Kemudian variabel y berisi label dari data ini('Revenue').

* train_test_split digunakan untuk membagi data menjadi 2 bagian yaitu train (data latih) dan test (data uji).

* Digunakan random state 42.

* test_size menentukan proporsi data yang akan dialokasikan untuk set pengujian. Disini digunakan 0.2 berarti 20% dari data akan digunakan untuk pengujian, sementara 80% akan digunakan untuk pelatihan.


"""

X_train_stratified, X_test_stratified, y_train_stratified, y_test_stratified = train_test_split(X, y,stratify=y, test_size=.2, random_state=42)

"""* Variabel ini akan berisi data yang telah dibagi menjadi set pelatihan dan set pengujian menggunakan strategi stratifikasi. Stratifikasi disini digunakan untuk memastikan bahwa distribusi kelas pada set pelatihan dan set pengujian tetap seimbang, karena pada data ini kelas 'revenue' tidak seimbang.

* train_test_split digunakan untuk membagi data menjadi 2 bagian yaitu train (data latih) dan test (data uji).

* Digunakan random state 42.

* test_size menentukan proporsi data yang akan dialokasikan untuk set pengujian. Disini digunakan 0.2 berarti 20% dari data akan digunakan untuk pengujian, sementara 80% akan digunakan untuk pelatihan.

"""

scaler = StandardScaler()
X_train_stratified = scaler.fit_transform(X_train_stratified)
X_test_stratified = scaler.transform(X_test_stratified)

"""fungsi StandarScaler untuk melakukan normalisasi (scaling) pada data. Normalisasi adalah proses mengubah fitur-fitur data sehingga memiliki rata-rata 0 dan standar deviasi 1.

# **9. Model Development**

BeberapaModel machine learning yang akan digunakan yaitu algoritma Decision Tree, Gradient Boosting Machines (GBM), Random Forest dan XgBoost.

## **9.1 Decision Tree**

Decision Tree (Pohon Keputusan) merupakan salah satu cara data processing dalam memprediksi masa depan dengan cara membangun klasifikasi atau regresi model dalam bentuk struktur pohon. Hal tersebut dilakukan dengan cara memecah terus ke dalam himpunan bagian yang lebih kecil lalu pada saat itu juga sebuah pohon keputusan secara bertahap dikembangkan. Hasil akhir dari proses tersebut adalah pohon dengan node keputusan dan node daun.
"""

# Inisialisasi dan Melatih Model Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train_stratified, y_train_stratified)

# Mendapatkan Feature Importances
importances_dt = dt_model.feature_importances_
features_dt = pd.DataFrame({'Feature': X.columns, 'Importance': importances_dt})
features_dt = features_dt.sort_values(by='Importance', ascending=False)

# Visualisasi Feature Importances - Decision Tree
plt.figure(figsize=(10, 6))
plt.barh(features_dt['Feature'], features_dt['Importance'], color='blue')
plt.xlabel('Importance')
plt.title('Feature Importances - Decision Tree')
plt.gca().invert_yaxis()
plt.show()

"""## **9.2 Random Forest**

Random Forest adalah algoritma supervised learning yang digunakan untuk klasifikasi dan regresi. Ini termasuk dalam kategori ensemble learning, di mana sekelompok model (dalam hal ini, decision trees) bekerja bersama untuk meningkatkan akurasi prediksi. Setiap model dalam Random Forest dilatih pada subset acak dari data dan fitur, lalu hasil prediksi dari semua model digabungkan untuk menghasilkan prediksi akhir.

Random Forest menggunakan teknik bagging (bootstrap aggregating) yang melibatkan pelatihan beberapa decision trees pada sampel data yang berbeda untuk mengurangi overfitting dan meningkatkan stabilitas model.
"""

# Inisialisasi dan Melatih Model Random Forest
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train_stratified, y_train_stratified)

# Mendapatkan Feature Importances
importances_rfc = rfc.feature_importances_
features_rfc = pd.DataFrame({'Feature': X.columns, 'Importance': importances_rfc})
features_rfc = features_rfc.sort_values(by='Importance', ascending=False)

# Visualisasi Feature Importances - Random Forest
plt.figure(figsize=(10, 6))
plt.barh(features_rfc['Feature'], features_rfc['Importance'], color='orange')
plt.xlabel('Importance')
plt.title('Feature Importances - Random Forest')
plt.gca().invert_yaxis()
plt.show()

"""## **9.3 Gradient Boosting Machines**

Gradient Boosting Machine (GBM) adalah metode ensemble learning yang menggabungkan beberapa model sederhana untuk meningkatkan performa prediksi. GBM bekerja dengan cara membangun model secara bertahap, di mana setiap model baru memperbaiki kesalahan prediksi model sebelumnya. Pada setiap iterasi, model baru ditambahkan untuk memperbaiki hasil prediksi, sehingga secara bertahap meningkatkan akurasi model secara keseluruhan. Pendekatan GBM bersifat "greedy" karena terus menyesuaikan prediksi hingga mencapai performa optimal.
"""

# Inisialisasi dan Melatih Model GBM
gbm_model = GradientBoostingClassifier(random_state=42)
gbm_model.fit(X_train_stratified, y_train_stratified)

# Mendapatkan Feature Importances
importances_gbm = gbm_model.feature_importances_
features_gbm = pd.DataFrame({'Feature': X.columns, 'Importance': importances_gbm})
features_gbm = features_gbm.sort_values(by='Importance', ascending=False)

# Visualisasi Feature Importances - GBM
plt.figure(figsize=(10, 6))
plt.barh(features_gbm['Feature'], features_gbm['Importance'], color='lightgreen')
plt.xlabel('Importance')
plt.title('Feature Importances - Gradient Boosting')
plt.gca().invert_yaxis()
plt.show()

"""## **9.4 XGBoost (Extreme Gradient Boosting)**

XGBoost (Extreme Gradient Boosting) adalah algoritma supervised learning yang sangat efektif dan populer untuk tugas klasifikasi dan regresi. XGBoost merupakan implementasi dari teknik gradient boosting yang mengoptimalkan dan mempercepat proses boosting model.
"""

# Inisialisasi dan Melatih Model XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_stratified, y_train_stratified)

# Mendapatkan Feature Importances
importances_xgb = xgb_model.feature_importances_
features_xgb = pd.DataFrame({'Feature': X.columns, 'Importance': importances_xgb})
features_xgb = features_xgb.sort_values(by='Importance', ascending=False)

# Visualisasi Feature Importances - XGBoost
plt.figure(figsize=(10, 6))
plt.barh(features_xgb['Feature'], features_xgb['Importance'], color='purple')
plt.xlabel('Importance')
plt.title('Feature Importances - XGBoost')
plt.gca().invert_yaxis()
plt.show()

"""## **9.5 Perbandingan Feature Important**"""

# Visualisasi Perbandingan Feature Importances
comparison_df.plot(kind='barh', x='Feature', figsize=(12, 8), color=['blue', 'orange', 'lightgreen', 'purple'])
plt.xlabel('Importance')
plt.title('Comparison of Feature Importances across Models')
plt.gca().invert_yaxis() # Membalik sumbu Y agar fitur terpenting di atas
plt.show()

"""Terlihat 3 fitur teratas yang paling mempengaruhi revenue dari pelanggan yaitu **PageValues, ProductRelated_Duration, dan ExitRates**

* PageValues: Fitur ini sangat penting karena mencerminkan nilai rata-rata halaman yang dikunjungi oleh pelanggan. Semakin tinggi nilai ini, semakin besar kemungkinan pelanggan akan melakukan pembelian (revenue). Hal ini karena PageValues menunjukkan kontribusi halaman tertentu terhadap transaksi.

* ProductRelated_Duration: Durasi yang dihabiskan oleh pengguna pada halaman-halaman terkait produk. Semakin lama seorang pelanggan menghabiskan waktu untuk melihat produk, semakin besar kemungkinan mereka tertarik untuk membeli, yang akhirnya berdampak positif terhadap revenue.

* ExitRates: Persentase pengunjung yang meninggalkan situs dari halaman tertentu. ExitRates yang lebih tinggi bisa menjadi indikator negatif, di mana pelanggan cenderung meninggalkan situs tanpa melakukan pembelian. Namun, ExitRates yang lebih rendah pada halaman penting menunjukkan potensi yang lebih besar untuk menghasilkan revenue.

# **10. Evaluasi**

Beberapa Evaluasi model yang digunakan yaitu menggunakan:
* ROC AUC (Receiver Operating Characteristic Area Under the Curve): Nilai ini menunjukkan seberapa baik model dapat membedakan antara kelas positif dan negatif. Semakin mendekati 1, semakin baik model.
Visualisasi ROC Curve: Membandingkan dua model dalam hal trade-off antara True Positive Rate (TPR) dan False Positive Rate (FPR).

* Confusion Matrix:
   * True Positives (TP): Kasus positif yang benar-benar diprediksi sebagai positif.
   * False Positives (FP): Kasus negatif yang salah diprediksi sebagai positif.
   * True Negatives (TN): Kasus negatif yang benar-benar diprediksi sebagai negatif.
   * False Negatives (FN): Kasus positif yang salah diprediksi sebagai negatif.

* Classification Report:
  * Akurasi : mengukur sejauh mana model berhasil memprediksi dengan benar secara keseluruhan. TP+TN/(TP+TN+FP+FN).
  * Presisi : mengukur seberapa akurat model dalam memprediksi kelas positif. TP/(TP+FP).
  * Recall : mengukur sejauh mana model klasifikasi dapat mengidentifikasi semua kasus positif yang sebenarnya. TP/(TP+FP)
  * F1 Score : Perbandingan Presisi dan Recall.

## **10.1 Evaluasi Agoritma Decision Tree**
"""

# Evaluasi Model - Decision Tree
y_pred_proba_dt = dt_model.predict_proba(X_test_stratified)[:, 1]
fpr_dt, tpr_dt, _ = roc_curve(y_test_stratified, y_pred_proba_dt)
roc_auc_dt = auc(fpr_dt, tpr_dt)

# ROC Curve - Decision Tree
plt.figure(figsize=(10, 6))
plt.plot(fpr_dt, tpr_dt, color='blue', lw=2, label='ROC curve (area = %0.3f)' % roc_auc_dt)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Decision Tree')
plt.legend(loc='lower right')
plt.show()

# Confusion Matrix - Decision Tree
conf_matrix_dt = confusion_matrix(y_test_stratified, dt_model.predict(X_test_stratified))
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Decision Tree')
plt.show()

# Classification Report - Decision Tree
print("Classification Report - Decision Tree:")
print(classification_report(y_test_stratified, dt_model.predict(X_test_stratified), digits=3))

"""## **10.2 Evaluasi Algoritma Random Forest**"""

# Evaluasi Model - Random Forest
y_pred_proba_rfc = rfc.predict_proba(X_test_stratified)[:, 1]
fpr_rfc, tpr_rfc, _ = roc_curve(y_test_stratified, y_pred_proba_rfc)
roc_auc_rfc = auc(fpr_rfc, tpr_rfc)

# ROC Curve - Random Forest
plt.figure(figsize=(10, 6))
plt.plot(fpr_rfc, tpr_rfc, color='darkorange', lw=2, label='ROC curve (area = %0.3f)' % roc_auc_rfc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Random Forest')
plt.legend(loc='lower right')
plt.show()

# Confusion Matrix - Random Forest
conf_matrix_rfc = confusion_matrix(y_test_stratified, rfc.predict(X_test_stratified))
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

# Classification Report - Random Forest
print("Classification Report - Random Forest:")
print(classification_report(y_test_stratified, rfc.predict(X_test_stratified), digits=3))

"""## **10.3 Evaluasi Algoritma Gradient Boosting Machine (GBM)**"""

# Evaluasi Model - GBM
y_pred_proba_gbm = gbm_model.predict_proba(X_test_stratified)[:, 1]
fpr_gbm, tpr_gbm, _ = roc_curve(y_test_stratified, y_pred_proba_gbm)
roc_auc_gbm = auc(fpr_gbm, tpr_gbm)

# ROC Curve - GBM
plt.figure(figsize=(10, 6))
plt.plot(fpr_gbm, tpr_gbm, color='green', lw=2, label='ROC curve (area = %0.3f)' % roc_auc_gbm)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Gradient Boosting')
plt.legend(loc='lower right')
plt.show()

# Confusion Matrix - GBM
conf_matrix_gbm = confusion_matrix(y_test_stratified, gbm_model.predict(X_test_stratified))
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_gbm, annot=True, fmt='d', cmap='Greens', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Gradient Boosting')
plt.show()

# Classification Report - GBM
print("Classification Report - Gradient Boosting:")
print(classification_report(y_test_stratified, gbm_model.predict(X_test_stratified), digits=3))

"""##  **10.4 Evaluasi Algoritma XgBoosting**"""

# Evaluasi Model - XGBoost
y_pred_proba_xgb = xgb_model.predict_proba(X_test_stratified)[:, 1]
fpr_xgb, tpr_xgb, _ = roc_curve(y_test_stratified, y_pred_proba_xgb)
roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

# ROC Curve - XGBoost
plt.figure(figsize=(10, 6))
plt.plot(fpr_xgb, tpr_xgb, color='purple', lw=2, label='ROC curve (area = %0.3f)' % roc_auc_xgb)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - XGBoost')
plt.legend(loc='lower right')
plt.show()

# Confusion Matrix - XGBoost
conf_matrix_xgb = confusion_matrix(y_test_stratified, xgb_model.predict(X_test_stratified))
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Purples', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - XGBoost')
plt.show()

# Classification Report - XGBoost
print("Classification Report - XGBoost:")
print(classification_report(y_test_stratified, xgb_model.predict(X_test_stratified), digits=3))

"""## **10.5 Perbandingan ROC AUC CURVE**"""

# Plot ROC Curve Comparison
plt.figure(figsize=(10, 6))

# ROC Curve - Decision Tree
plt.plot(fpr_dt, tpr_dt, color='blue', lw=2, label='Decision Tree (AUC = %0.3f)' % roc_auc_dt)

# ROC Curve - Random Forest
plt.plot(fpr_rfc, tpr_rfc, color='darkorange', lw=2, label='Random Forest (AUC = %0.3f)' % roc_auc_rfc)

# ROC Curve - Gradient Boosting
plt.plot(fpr_gbm, tpr_gbm, color='green', lw=2, label='GBM (AUC = %0.3f)' % roc_auc_gbm)

# ROC Curve - XGBoost
plt.plot(fpr_xgb, tpr_xgb, color='purple', lw=2, label='XGBoost (AUC = %0.3f)' % roc_auc_xgb)

# Baseline (diagonal line)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

# Plot settings
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')
plt.show()

"""Berdasarkan ROC AUC Curve dimana semakin mendekati nilai 1, maka model semakin baik terlihat bahwa Gradient Boosting Machines adalah algoritma yang paling baik dalam membedakan antara kelas True Positive dan Fale Positive dengan nilai 0,914.

## **10.6 Perbandingan Confussion Matrix**
"""

# Initialize the figure
fig, axs = plt.subplots(2, 2, figsize=(15, 12))

# Confusion Matrix - Decision Tree
conf_matrix_dt = confusion_matrix(y_test_stratified, dt_model.predict(X_test_stratified))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Oranges', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axs[1, 0])
axs[1, 0].set_title('Confusion Matrix - Decision Tree')
axs[1, 0].set_xlabel('Predicted')
axs[1, 0].set_ylabel('Actual')

# Confusion Matrix - Random Forest
conf_matrix_rfc = confusion_matrix(y_test_stratified, rfc.predict(X_test_stratified))
sns.heatmap(conf_matrix_rfc, annot=True, fmt='d', cmap='pink', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axs[0, 1])
axs[0, 1].set_title('Confusion Matrix - Random Forest')
axs[0, 1].set_xlabel('Predicted')
axs[0, 1].set_ylabel('Actual')

# Confusion Matrix - Gradient Boosting
conf_matrix_gbm = confusion_matrix(y_test_stratified, gbm_model.predict(X_test_stratified))
sns.heatmap(conf_matrix_gbm, annot=True, fmt='d', cmap='Greens', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axs[0, 0])
axs[0, 0].set_title('Confusion Matrix - Gradient Boosting')
axs[0, 0].set_xlabel('Predicted')
axs[0, 0].set_ylabel('Actual')

# Confusion Matrix - XGBoost
conf_matrix_xgb = confusion_matrix(y_test_stratified, xgb_model.predict(X_test_stratified))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Purples', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axs[1, 1])
axs[1, 1].set_title('Confusion Matrix - XGBoost')
axs[1, 1].set_xlabel('Predicted')
axs[1, 1].set_ylabel('Actual')

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

"""* Model Gradient Boosting terlihat cukup efektif dalam hal False Positive, dengan hanya 93 prediksi salah. Model ini juga memiliki jumlah True Positive yang tinggi (227) dibanding beberapa model lain, menunjukkan  keseimbangan yang baik dalam mendeteksi kedua kelas.

* Random Forest sedikit kurang akurat dibanding Gradient Boosting, dengan lebih banyak False Positives dan False Negatives. Namun, secara umum, model ini masih cukup kuat dalam memprediksi True Negatives dan True Positives dengan performa yang seimbang.

* Decision Tree menghasilkan lebih banyak False Positives (179) dibandingkan dengan Gradient Boosting dan Random Forest, yang menunjukkan bahwa model ini mungkin terlalu sering memprediksi kelas Positive. Ini mungkin membuat model kurang baik dalam mengidentifikasi kelas negatif yang benar.

* XGBoost menunjukkan performa yang mirip dengan Gradient Boosting,tapi lebih bagus performa dadari xgboost dalam prediksi kelasnya.

## **10.7 Perbandingan Classification Report**
"""

# Classification Report - Decision Tree
print("Classification Report - Decision Tree:")
print(classification_report(y_test_stratified, dt_model.predict(X_test_stratified), digits=3))

# Classification Report - Random Forest
print("\nClassification Report - Random Forest:")
print(classification_report(y_test_stratified, rfc.predict(X_test_stratified), digits=3))

# Classification Report - Gradient Boosting
print("\nClassification Report - Gradient Boosting:")
print(classification_report(y_test_stratified, gbm_model.predict(X_test_stratified), digits=3))

# Classification Report - XGBoost
print("\nClassification Report - XGBoost:")
print(classification_report(y_test_stratified, xgb_model.predict(X_test_stratified), digits=3))

"""* Decision Tree memiliki akurasi yang cukup baik (85.5%), namun performa untuk kelas 1 (dengan precision 53,6% dan recall 54,2%) menunjukkan bahwa model ini kesulitan mengidentifikasi kasus positif (kelas 1). F1-Score yang rendah untuk kelas 1 juga menunjukkan ketidakseimbangan dalam prediksi.

* Random Forest memberikan peningkatan akurasi (88.7%) dibandingkan Decision Tree. Precision dan F1-Score untuk kelas 1 meningkat signifikan (precision 66,7% dan F1-Score 60,3%), menunjukkan bahwa model ini lebih baik dalam mengidentifikasi kasus positif (kelas 1). Namun, recall untuk kelas 1 (55%) masih rendah, artinya model ini tidak dapat mendeteksi semua kasus positif.

* Gradient Boosting memberikan hasil terbaik dari segi accuracy (89.8%) dan precision untuk kelas 1 (70,9%). Namun, recall untuk kelas 1 masih lebih rendah (59,4%), meskipun sedikit lebih baik daripada Random Forest. F1-Score untuk kelas 1 juga meningkat menjadi 64,7%, menunjukkan kemampuan model yang lebih baik dalam mengidentifikasi kasus positif secara keseluruhan.

* XGBoost menunjukkan performa yang mendekati Gradient Boosting, dengan accuracy yang tinggi (89.3%). Precision untuk kelas 1 (68,6%) dan recall (57,9%) masih lebih baik dibandingkan Random Forest, tetapi sedikit lebih rendah dibandingkan Gradient Boosting. Namun, secara keseluruhan model ini juga memberikan keseimbangan yang cukup baik dalam memprediksi kedua kelas.

# **INTERPRETASI**

**KESIMPULAN**

**Berdasarkan analisis dan model machine learning 3 fitur yang paling berpengaruh dalam menghasilkan Revenue oleh Pelanggan yaitu PageValues, ProductRelated_Duration, dan ExitRates**

**Kemudan Algoritma Machine Learning terbaik untuk memprediksi revenue oleh pelanggan yaitu Gradien Boosting Machines (GBM) dengan nilai ROC AUC: 0,914 ini berarti model sangat baik dalam membedakan kelas positif dan negatif. Akurasi dari GBM paling tinggi dari model lainyya yaitu 89,8%.**
"""